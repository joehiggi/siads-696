{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7e1f07",
   "metadata": {},
   "source": [
    "# Foreign Exchange Return Forecasting of Neighboring Countries based on Powerful Anchor Countries\n",
    "# *SIADS 696: Milestone II*\n",
    "\n",
    "### *By Team #2*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd59bc",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Defining Custom Functions\n",
    "- Importing Data\n",
    "- Applying Unsupervised Learning\n",
    "- Applying Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a791312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import country_converter as coco\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Importing partial packages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c3aa1",
   "metadata": {},
   "source": [
    "## Defining Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ca5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing column names to standardize\n",
    "def standardize_column_names(dataframe):\n",
    "    dataframe.columns = [\n",
    "        str(column).lower().replace(\" \", \"_\").replace(\",\", \"\")\n",
    "        for column in dataframe.columns\n",
    "    ]\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29dfcc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(country):\n",
    "    if pd.isna(country):\n",
    "        return country\n",
    "    country = str(country).strip()\n",
    "    country = p.sub(\"\", country)\n",
    "    return re.sub(r\",\\s*The$\", \"\", country, flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1cf2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short(country):\n",
    "    if pd.isna(country):\n",
    "        return pd.NA\n",
    "    res = cc.convert(names=clean(country), to=\"name_short\")\n",
    "    if isinstance(res, (list, tuple)):\n",
    "        res = res[0] if res else pd.NA\n",
    "    if not res or res == \"not found\" or pd.isna(res):\n",
    "        warnings.warn(f\"Problem: {country}\")\n",
    "        return pd.NA\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48a9ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xxxusd(code):\n",
    "    x = f\"{code}USD=X\", f\"USD{code}=X\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b03401",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cc415f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>slug</th>\n",
       "      <th>value</th>\n",
       "      <th>date_of_information</th>\n",
       "      <th>ranking</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>china</td>\n",
       "      <td>$33,598,000,000,000</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>East and Southeast Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>united-states</td>\n",
       "      <td>$25,676,000,000,000</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India</td>\n",
       "      <td>india</td>\n",
       "      <td>$14,244,000,000,000</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>South Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Russia</td>\n",
       "      <td>russia</td>\n",
       "      <td>$6,089,000,000,000</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>Central Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japan</td>\n",
       "      <td>japan</td>\n",
       "      <td>$5,715,000,000,000</td>\n",
       "      <td>2024</td>\n",
       "      <td>5</td>\n",
       "      <td>East and Southeast Asia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name           slug                value  date_of_information  \\\n",
       "0          China          china  $33,598,000,000,000                 2024   \n",
       "1  United States  united-states  $25,676,000,000,000                 2024   \n",
       "2          India          india  $14,244,000,000,000                 2024   \n",
       "3         Russia         russia   $6,089,000,000,000                 2024   \n",
       "4          Japan          japan   $5,715,000,000,000                 2024   \n",
       "\n",
       "   ranking                   region  \n",
       "0        1  East and Southeast Asia  \n",
       "1        2            North America  \n",
       "2        3               South Asia  \n",
       "3        4             Central Asia  \n",
       "4        5  East and Southeast Asia  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Real GDP Purchasing Power Parity\n",
    "# Note the Ranking; 1 = big country per region\n",
    "country_gdp = pd.read_csv(\"../data/input/real_gdp_purchasing_power_parity_0.csv\")\n",
    "\n",
    "# Displaying the first 5 rows of the dataframe\n",
    "country_gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5e8d946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>world_gdp</th>\n",
       "      <th>time_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.366519e+12</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.444156e+12</td>\n",
       "      <td>1961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.545552e+12</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.671283e+12</td>\n",
       "      <td>1963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.832063e+12</td>\n",
       "      <td>1964</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      world_gdp  time_period\n",
       "0  1.366519e+12         1960\n",
       "1  1.444156e+12         1961\n",
       "2  1.545552e+12         1962\n",
       "3  1.671283e+12         1963\n",
       "4  1.832063e+12         1964"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Fred Anchors\n",
    "df_fred_world_gdp = pd.read_csv(\"../data/input/fred_anchors_0.csv\")\n",
    "\n",
    "# Preprocessing Fred Anchors\n",
    "df_fred_world_gdp[\"observation_date\"] = pd.to_datetime(\n",
    "    df_fred_world_gdp[\"observation_date\"]\n",
    ")\n",
    "df_fred_world_gdp[\"time_period\"] = df_fred_world_gdp[\"observation_date\"].dt.year\n",
    "df_fred_world_gdp = df_fred_world_gdp.drop(columns=[\"observation_date\"]).rename(\n",
    "    columns={\"NYGDPMKTPCDWLD\": \"world_gdp\"}\n",
    ")\n",
    "\n",
    "# Displaying the first 5 rows of the dataframe\n",
    "df_fred_world_gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d496e3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Importing GDP\u001b[39;00m\n\u001b[32m      2\u001b[39m df_imf_gdp = pd.read_csv(\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m../data/input/imf_gdp_0.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     on_bad_lines=\u001b[33m\"\u001b[39m\u001b[33mskip\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     sep=\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     quoting=\u001b[43mcsv\u001b[49m.QUOTE_ALL,\n\u001b[32m      7\u001b[39m     skipinitialspace=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      8\u001b[39m     usecols=[\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCOUNTRY\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTIME_PERIOD\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTYPE_OF_TRANSFORMATION\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFREQUENCY\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOBS_VALUE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mINDICATOR\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m     ],\n\u001b[32m     16\u001b[39m     engine=\u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Preprocessing GDP\u001b[39;00m\n\u001b[32m     20\u001b[39m df_imf_gdp = (\n\u001b[32m     21\u001b[39m     standardize_column_names(df_imf_gdp)\n\u001b[32m     22\u001b[39m     .query(\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     .reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     33\u001b[39m ).assign(year=\u001b[38;5;28;01mlambda\u001b[39;00m d: pd.to_datetime(d[\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m], errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m).dt.year)\n",
      "\u001b[31mNameError\u001b[39m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing GDP\n",
    "df_imf_gdp = pd.read_csv(\n",
    "    \"../data/input/imf_gdp_0.csv\",\n",
    "    on_bad_lines=\"skip\",\n",
    "    sep=\",\",\n",
    "    quoting=csv.QUOTE_ALL,\n",
    "    skipinitialspace=True,\n",
    "    usecols=[\n",
    "        \"COUNTRY\",\n",
    "        \"TIME_PERIOD\",\n",
    "        \"TYPE_OF_TRANSFORMATION\",\n",
    "        \"FREQUENCY\",\n",
    "        \"OBS_VALUE\",\n",
    "        \"INDICATOR\",\n",
    "    ],\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# Preprocessing GDP\n",
    "df_imf_gdp = (\n",
    "    standardize_column_names(df_imf_gdp)\n",
    "    .query(\n",
    "        \"indicator == 'US Dollar per domestic currency' \"\n",
    "        \"and type_of_transformation == 'End-of-period (EoP)' \"\n",
    "        \"and frequency == 'Annual'\"\n",
    "    )\n",
    "    .dropna(subset=[\"obs_value\"])\n",
    "    .rename(columns={\"time_period\": \"year\"})\n",
    "    .assign(year=lambda d: d[\"year\"].str[:4])\n",
    "    .sort_values([\"country\", \"year\"])\n",
    "    .drop(columns=[\"indicator\", \"type_of_transformation\", \"frequency\"])\n",
    "    .reset_index(drop=True)\n",
    ").assign(year=lambda d: pd.to_datetime(d[\"year\"], errors=\"coerce\").dt.year)\n",
    "\n",
    "# Displaying the first 5 rows of the dataframe\n",
    "df_imf_gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Exchange Rates\n",
    "df_imf_trade = pd.read_csv(\n",
    "    \"../data/input/imf_trade_0.csv\",\n",
    "    usecols=[\n",
    "        \"COUNTRY\",\n",
    "        \"COUNTERPART_COUNTRY\",\n",
    "        \"TIME_PERIOD\",\n",
    "        \"OBS_VALUE\",\n",
    "        \"TRADE_FLOW\",\n",
    "        \"SCALE\",\n",
    "        \"UNIT\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Manually excluding countries that are either trade in pegged currencies or don't have a true boundary from another\n",
    "ls_0 = [\n",
    "    \"World\",\n",
    "    \"Advanced Economies\",\n",
    "    \"Latin America and the Caribbean (LAC)\",\n",
    "    \"Hong Kong Special Administrative Region, People's Republic of China\",\n",
    "    \"Emerging and Developing Europe\",\n",
    "    \"Middle East and Central Asia\",\n",
    "    \"Emerging Market and Developing Economies\",\n",
    "    \"Euro Area (EA)\",\n",
    "    \"Emerging and Developing Asia\",\n",
    "]\n",
    "\n",
    "# Excluding the \"Exclusion List\"\n",
    "df_imf_trade = df_imf_trade[\n",
    "    (~df_imf_trade[\"COUNTRY\"].isin(ls_0))\n",
    "    & (~df_imf_trade[\"COUNTERPART_COUNTRY\"].isin(ls_0))\n",
    "]\n",
    "\n",
    "# Viewing first five rows\n",
    "df_imf_trade = standardize_column_names(df_imf_trade).sort_values(\n",
    "    by=[\"obs_value\"], ascending=False\n",
    ")\n",
    "\n",
    "# Displaying the first 5 rows of the dataframe\n",
    "df_imf_trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bda0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining countries in the European Union that use the Euro (EUR) and British Pound (GBP)\n",
    "EUR = [\n",
    "    \"Austria\",\n",
    "    \"Belgium\",\n",
    "    \"Cyprus\",\n",
    "    \"Estonia\",\n",
    "    \"Finland\",\n",
    "    \"France\",\n",
    "    \"Germany\",\n",
    "    \"Greece\",\n",
    "    \"Ireland\",\n",
    "    \"Italy\",\n",
    "    \"Latvia\",\n",
    "    \"Lithuania\",\n",
    "    \"Luxembourg\",\n",
    "    \"Malta\",\n",
    "    \"Netherlands\",\n",
    "    \"Portugal\",\n",
    "    \"Slovakia\",\n",
    "    \"Slovenia\",\n",
    "    \"Spain\",\n",
    "    \"Andorra\",\n",
    "    \"Monaco\",\n",
    "    \"San Marino\",\n",
    "    \"Vatican City\",\n",
    "    \"Saint Barthélemy\",\n",
    "    \"Saint Pierre and Miquelon\",\n",
    "    \"Kosovo\",\n",
    "    \"Montenegro\",\n",
    "    \"Bosnia and Herzegovina\",\n",
    "    \"Bulgaria\",\n",
    "    \"Cape Verde\",\n",
    "    \"Cameroon\",\n",
    "    \"Central African Republic\",\n",
    "    \"Chad\",\n",
    "    \"Republic of the Congo\",\n",
    "    \"Equatorial Guinea\",\n",
    "    \"Gabon\",\n",
    "    \"Benin\",\n",
    "    \"Burkina Faso\",\n",
    "    \"Côte d'Ivoire\",\n",
    "    \"Guinea-Bissau\",\n",
    "    \"Mali\",\n",
    "    \"Niger\",\n",
    "    \"Senegal\",\n",
    "    \"Togo\",\n",
    "    \"French Polynesia\",\n",
    "    \"New Caledonia\",\n",
    "    \"Wallis and Futuna\",\n",
    "    \"Comoros\",\n",
    "    \"Croatia\",\n",
    "    \"Morocco\",\n",
    "    \"São Tomé and Príncipe\",\n",
    "    \"Denmark\",\n",
    "    \"North Macedonia\",\n",
    "]\n",
    "\n",
    "# Defining countries that use the British Pound (GBP)\n",
    "GBP = [\n",
    "    \"Guernsey\",\n",
    "    \"Jersey\",\n",
    "    \"Isle of Man\",\n",
    "    \"Gibraltar\",\n",
    "    \"Falkland Islands\",\n",
    "    \"Saint Helena\",\n",
    "]\n",
    "\n",
    "# Defining other countries with pegged currencies or no true boundary\n",
    "other = [\n",
    "    \"Bhutan\",\n",
    "    \"Nepal\",\n",
    "    \"North Korea\",\n",
    "    \"Afghanistan\",\n",
    "    \"Turkmenistan\",\n",
    "    \"South Sudan\",\n",
    "    \"Guam\",\n",
    "    \"Macau\",\n",
    "    \"Tuvalu\",\n",
    "    \"Kiribati\",\n",
    "    \"Palau\",\n",
    "    \"Greenland\",\n",
    "    \"Maldives\",\n",
    "    \"Iraq\",\n",
    "    \"Solomon Islands\",\n",
    "    \"Brunei Darussalam\",\n",
    "    \"Bangladesh\",\n",
    "    \"Myanmar\",\n",
    "    \"Marshall Islands\",\n",
    "    \"Iran\",\n",
    "    \"Yemen\",\n",
    "    \"Libya\",\n",
    "    \"Somalia\",\n",
    "    \"Liberia\",\n",
    "    \"Sudan\",\n",
    "    \"Sierra Leone\",\n",
    "    \"Mongolia\",\n",
    "    \"Angola\",\n",
    "    \"Kyrgyz Republic\",\n",
    "    \"Tajikistan\",\n",
    "]\n",
    "\n",
    "# Cleaning country names\n",
    "EUR = {short(x) for x in EUR if pd.notna(short(x))}\n",
    "GBP = {short(x) for x in GBP if pd.notna(short(x))}\n",
    "other = {short(x) for x in other if pd.notna(short(x))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa99ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing cleaned data\n",
    "# TODO - where is this coming from? I.e. what code created these files?\n",
    "df_trade = pd.read_parquet(\"../data/input/trade_0.parquet\")\n",
    "df_gdp = pd.read_parquet(\"../data/input/gdp_0.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e13d9",
   "metadata": {},
   "source": [
    "## Applying Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95afa563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting country names to short format\n",
    "cc = coco.CountryConverter()\n",
    "p = re.compile(\n",
    "    r\",\\s*(?:Kingdom of the Netherlands|United Kingdom-British Overseas Territory|Republic of the|Union of the|State of the)$\",\n",
    "    re.I,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6196915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchors selection\n",
    "df_gdp = df_gdp[~(df_gdp[\"COUNTRY\"] == \"United States\")]\n",
    "df_gdp.head()\n",
    "Anchor = {\n",
    "    \"EUR\": EUR,\n",
    "    \"CNY\": {\"China\"},\n",
    "    \"JPY\": {\"Japan\"},\n",
    "}\n",
    "\n",
    "# No USD due to many relationship ties with multiple currency.\n",
    "# Time Period\n",
    "period = 10\n",
    "\n",
    "# N Neighbor\n",
    "n = 10\n",
    "\n",
    "# Minimum Percentage Volume\n",
    "v = 0.05\n",
    "\n",
    "# Exports of goods\n",
    "# Neighbor mapping\n",
    "neighbor = {x: i for i, y in Anchor.items() for x in y}\n",
    "ex = EUR | GBP | other | {\"China\", \"Japan\", \"United States\"}\n",
    "df_trade = df_trade.dropna(\n",
    "    subset=[\"COUNTRY\", \"COUNTERPART_COUNTRY\", \"TIME_PERIOD\", \"OBS_VALUE\"]\n",
    ").copy()\n",
    "\n",
    "# Keep data period\n",
    "df_trade = df_trade[df_trade[\"TIME_PERIOD\"].astype(str).str.match(r\"^\\d{4}\")]\n",
    "df_trade[\"year\"] = df_trade[\"TIME_PERIOD\"].astype(str).str[:4].astype(int)\n",
    "max_year = int(df_trade[\"year\"].max())\n",
    "df_trade = df_trade[df_trade[\"year\"].between(max_year - period + 1, max_year)]\n",
    "\n",
    "# Filter and map potential neighbors\n",
    "df_export = df_trade[~df_trade[\"COUNTRY\"].isin(ex)].copy()\n",
    "df_export[\"anchor\"] = df_export[\"COUNTERPART_COUNTRY\"].map(neighbor)\n",
    "\n",
    "# Total exports per country\n",
    "etot = (\n",
    "    df_export.groupby(\"COUNTRY\", as_index=False)[\"OBS_VALUE\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"OBS_VALUE\": \"export_total\"})\n",
    ")\n",
    "\n",
    "# Exports to each anchor\n",
    "eanch = (\n",
    "    df_export.dropna(subset=[\"anchor\"])\n",
    "    .groupby([\"COUNTRY\", \"anchor\"], as_index=False)[\"OBS_VALUE\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"OBS_VALUE\": \"export_to_anchor\"})\n",
    ")\n",
    "\n",
    "# Export shares\n",
    "esh = eanch.merge(etot, on=\"COUNTRY\", how=\"left\").assign(\n",
    "    export_share=lambda d: d[\"export_to_anchor\"] / d[\"export_total\"]\n",
    ")\n",
    "\n",
    "# Best export anchor per country\n",
    "bexp = esh.sort_values(\n",
    "    [\"COUNTRY\", \"export_share\", \"export_to_anchor\"], ascending=[True, False, False]\n",
    ").drop_duplicates(\"COUNTRY\")\n",
    "bexp = bexp[bexp[\"export_share\"] >= v]\n",
    "\n",
    "# Swap role\n",
    "df_import = df_trade.rename(\n",
    "    columns={\"COUNTRY\": \"COUNTERPART_COUNTRY_orig\", \"COUNTERPART_COUNTRY\": \"COUNTRY\"}\n",
    ").rename(columns={\"COUNTERPART_COUNTRY_orig\": \"COUNTERPART_COUNTRY\"})\n",
    "\n",
    "# Potential neighbors\n",
    "df_import = df_import[~df_import[\"COUNTRY\"].isin(ex)].copy()\n",
    "df_import[\"anchor\"] = df_import[\"COUNTERPART_COUNTRY\"].map(neighbor)\n",
    "\n",
    "# Total imports per country\n",
    "import_totals = (\n",
    "    df_import.groupby(\"COUNTRY\", as_index=False)[\"OBS_VALUE\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"OBS_VALUE\": \"import_total\"})\n",
    ")\n",
    "\n",
    "# Imports from each anchor\n",
    "ianch = (\n",
    "    df_import.dropna(subset=[\"anchor\"])\n",
    "    .groupby([\"COUNTRY\", \"anchor\"], as_index=False)[\"OBS_VALUE\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"OBS_VALUE\": \"import_from_anchor\"})\n",
    ")\n",
    "\n",
    "# Import shares\n",
    "ish = ianch.merge(import_totals, on=\"COUNTRY\", how=\"left\").assign(\n",
    "    import_share=lambda d: d[\"import_from_anchor\"] / d[\"import_total\"]\n",
    ")\n",
    "\n",
    "# Best import anchor per country\n",
    "bimp = ish.sort_values(\n",
    "    [\"COUNTRY\", \"import_share\", \"import_from_anchor\"], ascending=[True, False, False]\n",
    ").drop_duplicates(\"COUNTRY\")\n",
    "bimp = bimp[bimp[\"import_share\"] >= v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine table\n",
    "comb = esh.merge(ish, on=[\"COUNTRY\", \"anchor\"], how=\"outer\", suffixes=(\"_exp\", \"_imp\"))\n",
    "\n",
    "# Handle missing value\n",
    "f = [\n",
    "    \"export_to_anchor\",\n",
    "    \"export_total\",\n",
    "    \"export_share\",\n",
    "    \"import_from_anchor\",\n",
    "    \"import_total\",\n",
    "    \"import_share\",\n",
    "]\n",
    "for c in f:\n",
    "    if c not in comb.columns:\n",
    "        comb[c] = 0.0\n",
    "comb[f] = comb[f].fillna(0.0)\n",
    "\n",
    "# Combined metrics\n",
    "comb = comb.assign(\n",
    "    total_trade_with_anchor=lambda d: d[\"export_to_anchor\"] + d[\"import_from_anchor\"],\n",
    "    total_trade_volume=lambda d: d[\"export_total\"] + d[\"import_total\"],\n",
    ")\n",
    "comb[\"combined_exposure\"] = 0.0\n",
    "nz = comb[\"total_trade_volume\"] > 0\n",
    "comb.loc[nz, \"combined_exposure\"] = (\n",
    "    comb.loc[nz, \"total_trade_with_anchor\"] / comb.loc[nz, \"total_trade_volume\"]\n",
    ")\n",
    "comb_f = comb.loc[comb[\"combined_exposure\"] >= v].copy()\n",
    "\n",
    "# Rank neighbors per anchor\n",
    "comb_f = comb_f.sort_values(\n",
    "    by=[\n",
    "        \"anchor\",\n",
    "        \"combined_exposure\",\n",
    "        \"total_trade_with_anchor\",\n",
    "        \"export_share\",\n",
    "        \"import_share\",\n",
    "        \"COUNTRY\",\n",
    "    ],\n",
    "    ascending=[True, False, False, False, False, True],\n",
    ")\n",
    "\n",
    "# Rank within each anchor and take top n\n",
    "comb_f[\"rank_within_anchor\"] = comb_f.groupby(\"anchor\")[\"combined_exposure\"].rank(\n",
    "    method=\"first\", ascending=False\n",
    ")\n",
    "comb_top = comb_f.loc[comb_f[\"rank_within_anchor\"] <= n].copy()\n",
    "\n",
    "# Final neighbor\n",
    "neighbors_dict = comb_top.groupby(\"anchor\")[\"COUNTRY\"].apply(list).to_dict()\n",
    "\n",
    "# Summary results\n",
    "summary = (\n",
    "    comb_top[\n",
    "        [\n",
    "            \"anchor\",\n",
    "            \"COUNTRY\",\n",
    "            \"combined_exposure\",\n",
    "            \"export_share\",\n",
    "            \"import_share\",\n",
    "            \"export_to_anchor\",\n",
    "            \"import_from_anchor\",\n",
    "            \"export_total\",\n",
    "            \"import_total\",\n",
    "            \"total_trade_with_anchor\",\n",
    "            \"total_trade_volume\",\n",
    "            \"rank_within_anchor\",\n",
    "        ]\n",
    "    ]\n",
    "    .sort_values([\"anchor\", \"rank_within_anchor\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "res = summary.copy()\n",
    "for col in [\"combined_exposure\", \"export_share\", \"import_share\"]:\n",
    "    res[col] = (res[col] * 100).round(2)\n",
    "\n",
    "res_sorted = res.sort_values([\"anchor\", \"rank_within_anchor\"])\n",
    "anchor_neighbor = res_sorted.groupby(\"anchor\")[\"COUNTRY\"].apply(list).to_dict()\n",
    "\n",
    "for a, c in [(\"CNY\", \"Australia\"), (\"JPY\", \"Philippines\"), (\"JPY\", \"Vietnam\")]:\n",
    "    if a in anchor_neighbor and c in anchor_neighbor[a]:\n",
    "        anchor_neighbor[a].remove(c)\n",
    "\n",
    "anchor_neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3e8c5",
   "metadata": {},
   "source": [
    "*Performing Principle Component Analyis (PCA) to reduce the dimensionality of our training data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1db890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We identify the hour anchor: Anchor = {\"EUR\": EUR, \"CNY\": {\"China\"}, \"JPY\": {\"Japan\"}}\n",
    "\n",
    "# Going to start by pulling Anchor Countries\n",
    "# Codes are obtain from yahoo finance symbol\n",
    "# https://finance.yahoo.com/markets/currencies/\n",
    "\n",
    "fx_list = {\n",
    "    # Anchor currencies\n",
    "    \"CNY\": \"CNY=X\",  # China\n",
    "    \"EUR\": \"EUR=X\",  # Euro\n",
    "    \"JPY\": \"JPY=X\",  # Japan\n",
    "    # Neutral currencies\n",
    "    \"CAD\": \"CAD=X\",  # Canada\n",
    "    \"BRL\": \"BRL=X\",  # Brazil\n",
    "    \"MXN\": \"MXN=X\",  # Mexico\n",
    "    \"COP\": \"COP=X\",  # Colombia\n",
    "    \"PEN\": \"PEN=X\",  # Peru\n",
    "    \"NOK\": \"NOK=X\",  # Norway\n",
    "    \"ZAR\": \"ZAR=X\",  # South Africa\n",
    "    \"INR\": \"INR=X\",  # India\n",
    "    \"TRY\": \"TRY=X\",  # Turkey\n",
    "    \"EGP\": \"EGP=X\",  # Egypt\n",
    "    \"RUB\": \"RUB=X\",  # Russia\n",
    "    \"ILS\": \"ILS=X\",  # Israel\n",
    "    # CNY group\n",
    "    \"CDF\": \"CDF=X\",  # DR Congo\n",
    "    \"LAK\": \"LAK=X\",  # Laos\n",
    "    \"TZS\": \"TZS=X\",  # Tanzania\n",
    "    \"CLP\": \"CLP=X\",  # Chile\n",
    "    \"GNF\": \"GNF=X\",  # Guinea\n",
    "    \"PKR\": \"PKR=X\",  # Pakistan\n",
    "    \"PHP\": \"PHP=X\",  # Philippines\n",
    "    \"VND\": \"VND=X\",  # Vietnam\n",
    "    \"MRU\": \"MRU=X\",  # Mauritania\n",
    "    # EUR group\n",
    "    \"ALL\": \"ALL=X\",  # Albania\n",
    "    \"CZK\": \"CZK=X\",  # Czechia\n",
    "    \"TND\": \"TND=X\",  # Tunisia\n",
    "    \"RON\": \"RON=X\",  # Romania\n",
    "    \"HUF\": \"HUF=X\",  # Hungary\n",
    "    \"PLN\": \"PLN=X\",  # Poland\n",
    "    \"RSD\": \"RSD=X\",  # Serbia\n",
    "    \"SEK\": \"SEK=X\",  # Sweden\n",
    "    \"ISK\": \"ISK=X\",  # Iceland\n",
    "    \"DZD\": \"DZD=X\",  # Algeria\n",
    "    # JPY group\n",
    "    \"PGK\": \"PGK=X\",  # Papua New Guinea\n",
    "    \"TWD\": \"TWD=X\",  # Taiwan\n",
    "    \"THB\": \"THB=X\",  # Thailand\n",
    "    \"AUD\": \"AUD=X\",  # Australia\n",
    "    \"IDR\": \"IDR=X\",  # Indonesia\n",
    "    \"KRW\": \"KRW=X\",  # South Korea\n",
    "    \"MYR\": \"MYR=X\",  # Malaysia\n",
    "    \"NZD\": \"NZD=X\",  # New Zealand\n",
    "}\n",
    "\n",
    "# Period is the timeframe we want, we conclude for the time being, it will be 10 years\n",
    "PERIOD = \"10y\"\n",
    "\n",
    "# We conclude we will be looking at exchanges on a daily basis.\n",
    "INTERVAL = \"1d\"\n",
    "FFill_Limit = 3\n",
    "N_PCS_To_Cluster = 3\n",
    "N_Cluster = 3\n",
    "\n",
    "# Creating a list of tickers from the fx_list dictionary\n",
    "tickers = list(fx_list.values())\n",
    "\n",
    "# Creating a DataFrame by downloading historical FX data using yfinance\n",
    "yfinance_fx_raw = yf.download(\n",
    "    tickers, period=PERIOD, interval=INTERVAL, auto_adjust=None, progress=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bdfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove problem currency\n",
    "x = yfinance_fx_raw[\"Close\"]\n",
    "c = x.notna().sum().sort_values()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove currencies that have less than 2000 data points\n",
    "want = list(fx_list.keys())\n",
    "t = []\n",
    "invert = {}\n",
    "\n",
    "# Loop through each currency in the want list and get their corresponding ticker symbols\n",
    "for c in want:\n",
    "    t1, t2 = xxxusd(c)\n",
    "    t.extend([t1, t2])\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "t = list(dict.fromkeys(t))\n",
    "\n",
    "# Creating a DataFrame by downloading historical FX data using yfinance\n",
    "raw = yf.download(\n",
    "    t, period=PERIOD, interval=INTERVAL, auto_adjust=None, progress=False\n",
    ")[\"Close\"]\n",
    "\n",
    "# Creating a DataFrame to hold exchange rate levels and whether they need to be inverted\n",
    "level = {}\n",
    "for c in want:\n",
    "    t1, t2 = xxxusd(c)\n",
    "    if t1 in raw.columns and raw[t1].notna().sum() > 0:\n",
    "        level[c] = raw[t1]\n",
    "        invert[c] = False\n",
    "    elif t2 in raw.columns and raw[t2].notna().sum() > 0:\n",
    "        level[c] = 1.0 / raw[t2]\n",
    "        invert[c] = True\n",
    "\n",
    "# Creating a DataFrame from the level dictionary and sorting by index\n",
    "level = pd.DataFrame(level).sort_index()\n",
    "\n",
    "# Calculating log returns and handling missing values\n",
    "lreturn = np.log(level / level.shift(1))\n",
    "lreturn = lreturn.ffill(limit=3).dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0238da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the log returns\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(\n",
    "    scaler.fit_transform(lreturn),\n",
    "    index=lreturn.index,\n",
    "    columns=lreturn.columns,\n",
    ")\n",
    "\n",
    "# PCA and KMeans Clustering\n",
    "pca = PCA(n_components=2)\n",
    "Z = pca.fit_transform(X.T)\n",
    "\n",
    "# Creating a DataFrame for PCA results\n",
    "pca_df = pd.DataFrame(Z, index=X.columns, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "# KMeans Clustering\n",
    "k = 3\n",
    "km = KMeans(n_clusters=k, random_state=42)\n",
    "pca_df[\"cluster\"] = km.fit_predict(pca_df[[\"PC1\", \"PC2\"]].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addidtional pull for explained_variance_ratio_\n",
    "evr = pca.explained_variance_ratio_\n",
    "print(\"Explained variance ratio: \", evr, \" | Cumalative: \", evr.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597912e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping currencies based on economic ties\n",
    "g = {\n",
    "    \"CNY\": [\"CDF\", \"LAK\", \"TZS\", \"CLP\", \"GNF\", \"PKR\", \"PHP\", \"VND\", \"MRU\"],\n",
    "    \"EUR\": [\"ALL\", \"CZK\", \"TND\", \"RON\", \"HUF\", \"PLN\", \"RSD\", \"SEK\", \"ISK\", \"DZD\"],\n",
    "    \"JPY\": [\"PGK\", \"TWD\", \"THB\", \"AUD\", \"IDR\", \"KRW\", \"MYR\", \"NZD\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Function to assign group based on currency code\n",
    "def assign(cc):\n",
    "    for i, j in g.items():\n",
    "        if cc == i or cc in j:\n",
    "            return i\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "# Assigning group to each currency in the PCA DataFrame\n",
    "pca_df[\"group\"] = pca_df.index.map(assign)\n",
    "\n",
    "# Visualization of PCA Results\n",
    "col = {\"CNY\": \"red\", \"EUR\": \"blue\", \"INR\": \"green\", \"JPY\": \"orange\", \"other\": \"gray\"}\n",
    "plt.figure(figsize=(12, 8))\n",
    "for grp, sub in pca_df.groupby(\"group\"):\n",
    "    plt.scatter(\n",
    "        sub[\"PC1\"],\n",
    "        sub[\"PC2\"],\n",
    "        c=col.get(grp, \"black\"),\n",
    "        s=70,\n",
    "        alpha=0.85,\n",
    "        label=f\"{grp} (n={len(sub)})\",\n",
    "    )\n",
    "    for i, j in sub.iterrows():\n",
    "        plt.annotate(\n",
    "            i,\n",
    "            (j[\"PC1\"], j[\"PC2\"]),\n",
    "            xytext=(3, 3),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "plt.title(\"Currencies in PCA Space (standardized returns, XXX/USD)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59125e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Silhouette Score\n",
    "score = silhouette_score(pca_df[[\"PC1\", \"PC2\"]], pca_df[\"cluster\"])\n",
    "\n",
    "# Saving the log returns DataFrame to a Parquet file\n",
    "lreturn.to_parquet(\"fx_log_return.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e4ed2",
   "metadata": {},
   "source": [
    "## Applying Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd90a84",
   "metadata": {},
   "source": [
    "*Applying Granger causality test to determine if a causal realtionship is found in our anchor and neighboring currency returns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651123e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowKeyError",
     "evalue": "A type extension with name pandas.period already defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowKeyError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m lreturn = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/input/fx_log_return.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m g = {\u001b[33m\"\u001b[39m\u001b[33mCNY\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mCDF\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLAK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTZS\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCLP\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mGNF\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPKR\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPHP\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mVND\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMRU\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      4\u001b[39m      \u001b[33m\"\u001b[39m\u001b[33mEUR\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mALL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCZK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTND\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRON\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHUF\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPLN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRSD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSEK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mISK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDZD\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      5\u001b[39m      \u001b[33m\"\u001b[39m\u001b[33mJPY\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mPGK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTWD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTHB\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAUD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIDR\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mKRW\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMYR\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNZD\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgranger\u001b[39m(res, g, ml=\u001b[32m5\u001b[39m, a=\u001b[32m0.05\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/pandas/io/parquet.py:653\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    502\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs,\n\u001b[32m    511\u001b[39m ) -> DataFrame:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    514\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    651\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    656\u001b[39m         msg = (\n\u001b[32m    657\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    658\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/pandas/io/parquet.py:64\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m engine_class \u001b[38;5;129;01min\u001b[39;00m engine_classes:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m         error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/pandas/io/parquet.py:170\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension_types\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mself\u001b[39m.api = pyarrow\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/pandas/core/arrays/arrow/extension_types.py:59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# register the type with a dummy instance\u001b[39;00m\n\u001b[32m     58\u001b[39m _period_type = ArrowPeriodType(\u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[43mpyarrow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_extension_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_period_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mArrowIntervalType\u001b[39;00m(pyarrow.ExtensionType):\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, subtype, closed: IntervalClosedType) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     64\u001b[39m         \u001b[38;5;66;03m# attributes need to be set first before calling\u001b[39;00m\n\u001b[32m     65\u001b[39m         \u001b[38;5;66;03m# super init (as that calls serialize)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/pyarrow/types.pxi:2226\u001b[39m, in \u001b[36mpyarrow.lib.register_extension_type\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowKeyError\u001b[39m: A type extension with name pandas.period already defined"
     ]
    }
   ],
   "source": [
    "lreturn = pd.read_parquet(\"../data/input/fx_log_return.parquet\")\n",
    "\n",
    "g = {\n",
    "    \"CNY\": [\"CDF\", \"LAK\", \"TZS\", \"CLP\", \"GNF\", \"PKR\", \"PHP\", \"VND\", \"MRU\"],\n",
    "    \"EUR\": [\"ALL\", \"CZK\", \"TND\", \"RON\", \"HUF\", \"PLN\", \"RSD\", \"SEK\", \"ISK\", \"DZD\"],\n",
    "    \"JPY\": [\"PGK\", \"TWD\", \"THB\", \"AUD\", \"IDR\", \"KRW\", \"MYR\", \"NZD\"],\n",
    "}\n",
    "\n",
    "\n",
    "def granger(res, g, ml=5, a=0.05):\n",
    "    x = []\n",
    "    for i, j in g.items():\n",
    "        for k in j:\n",
    "            if i not in res.columns or k not in res.columns:\n",
    "                continue\n",
    "            df = pd.concat([res[k], res[i]], axis=1).dropna()\n",
    "            df.columns = [\"neighbor\", \"anchor\"]\n",
    "            if len(df) <= ml * 3:\n",
    "                continue\n",
    "            t = grangercausalitytests(df, maxlag=ml)\n",
    "            pval = [t[lag][0][\"ssr_ftest\"][1] for lag in range(1, ml + 1)]\n",
    "            min_p = float(np.min(pval))\n",
    "            best_lag = int(np.argmin(pval) + 1)\n",
    "            x.append(\n",
    "                {\n",
    "                    \"anchor\": i,\n",
    "                    \"neighbor\": k,\n",
    "                    \"min_pval\": round(min_p, 4),\n",
    "                    \"best_lag (days)\": best_lag,\n",
    "                    \"significant\": (min_p < a),\n",
    "                }\n",
    "            )\n",
    "    rdf = pd.DataFrame(x)\n",
    "    return rdf\n",
    "\n",
    "\n",
    "rdf = granger(lreturn, g, 10, 0.05)\n",
    "rdf = rdf.sort_values([\"anchor\", \"min_pval\"]).reset_index(drop=True)\n",
    "an = rdf[rdf[\"significant\"] == True].sort_values([\"anchor\", \"min_pval\"])\n",
    "ng = an.groupby(\"anchor\")[\"neighbor\"].apply(list).to_dict()\n",
    "ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4646da",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.12.6)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/josephhiggins/Documents/siads-696/venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "ng = {\"EUR\": [\"ALL\", \"CZK\", \"TND\", \"RON\", \"HUF\", \"PLN\", \"RSD\", \"SEK\", \"ISK\", \"DZD\"]}\n",
    "\n",
    "ls_0 = [i for i in ng.items() if i[0] == \"EUR\"][0]\n",
    "ls_1 = [ls_0[0] for i in range(len([i for i in ng.items() if i[0] == \"EUR\"][0][1]))]\n",
    "len(ls_0[1]), len(ls_1)\n",
    "\n",
    "pd.DataFrame({\"Anchor\": ls_1, \"Neighbor\": ls_0[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_0 = pd.read_parquet(\"../data/input/fx_log_return.parquet\")[[\"CNY\", \"CDF\"]]\n",
    "\n",
    "# Starting with CNY/CDF as an example\n",
    "df_0[\"target\"] = df_0[\"CDF\"].shift(-1)\n",
    "\n",
    "# Dropping all NA\n",
    "df_0.dropna(inplace=True)\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab891aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features (e.g., previous 5 days' returns)\n",
    "for i in range(1, 6):\n",
    "    df_0[f\"Return_lag_{i}\"] = df_0[\"CDF\"].pct_change().shift(i)\n",
    "\n",
    "# Create moving average features\n",
    "df_0[\"MA_10\"] = df_0[\"CDF\"].rolling(window=10).mean().shift(1)\n",
    "df_0[\"MA_50\"] = df_0[\"CDF\"].rolling(window=50).mean().shift(1)\n",
    "\n",
    "# Drop initial rows with NaNs created by rolling windows\n",
    "df_0.dropna(inplace=True)\n",
    "df_0 = df_0.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dabdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "features = [col for col in df_0.columns if col not in [\"target\"]]\n",
    "X = df_0[features]\n",
    "y = df_0[\"target\"]\n",
    "\n",
    "# Time-based train-test split\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "parameters = {\n",
    "    \"nthread\": [4],  # when use hyperthread, xgboost may become slower\n",
    "    \"objective\": [\"reg:squarederror\"],\n",
    "    \"learning_rate\": [0.03, 0.24, 0.8],\n",
    "    \"max_depth\": [5, 6, 7],\n",
    "    \"min_child_weight\": [4],\n",
    "    \"subsample\": [0.2, 0.5, 0.8],\n",
    "    \"colsample_bytree\": [0.7],\n",
    "    \"n_estimators\": [2, 5, 10],\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb, parameters, cv=2, n_jobs=5, verbose=True)\n",
    "\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
