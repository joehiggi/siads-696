{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b7e1f07",
   "metadata": {},
   "source": [
    "# Foreign Exchange Return Forecasting of Neighboring Countries based on Powerful Anchor Countries\n",
    "# *SIADS 696: Milestone II*\n",
    "\n",
    "### *By Team #2*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bd59bc",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- Defining Custom Functions\n",
    "- Importing Data\n",
    "- Applying Unsupervised Learning\n",
    "- Applying Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a791312a",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/josephhiggins/Documents/siads-696/env/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <B637898E-C0C3-3F93-8C08-800EE41A7A5B> /Users/josephhiggins/Documents/siads-696/env/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcountry_converter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcoco\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myfinance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myf\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/xgboost/__init__.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"XGBoost: eXtreme Gradient Boosting library.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mContributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tracker  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m collective\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     Booster,\n\u001b[32m     10\u001b[39m     DataIter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     build_info,\n\u001b[32m     16\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/xgboost/tracker.py:9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menum\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IntEnum, unique\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB, _check_call, _deprecate_positional_args, make_jcargs\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_family\u001b[39m(addr: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m     13\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get network family from address.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/xgboost/core.py:295\u001b[39m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# load the XGBoost library globally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m _LIB = \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_call\u001b[39m(ret: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    299\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    300\u001b[39m \n\u001b[32m    301\u001b[39m \u001b[33;03m    This function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    307\u001b[39m \u001b[33;03m        return value from API calls\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/xgboost/core.py:257\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_success:\n\u001b[32m    256\u001b[39m         libname = os.path.basename(lib_paths[\u001b[32m0\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(\n\u001b[32m    258\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[33mXGBoost Library (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlibname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) could not be loaded.\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[33mLikely causes:\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[33m  * OpenMP runtime is not installed\u001b[39m\n\u001b[32m    262\u001b[39m \u001b[33m    - vcomp140.dll or libgomp-1.dll for Windows\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[33m    - libomp.dylib for Mac OSX\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[33m    - libgomp.so for Linux and other UNIX-like OSes\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[33m    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\u001b[39m\n\u001b[32m    266\u001b[39m \n\u001b[32m    267\u001b[39m \u001b[33m  * You are running 32-bit Python on a 64-bit OS\u001b[39m\n\u001b[32m    268\u001b[39m \n\u001b[32m    269\u001b[39m \u001b[33mError message(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos_error_list\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m    271\u001b[39m         )\n\u001b[32m    272\u001b[39m     _register_log_callback(lib)\n\u001b[32m    274\u001b[39m     libver = _lib_version(lib)\n",
      "\u001b[31mXGBoostError\u001b[39m: \nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/josephhiggins/Documents/siads-696/env/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <B637898E-C0C3-3F93-8C08-800EE41A7A5B> /Users/josephhiggins/Documents/siads-696/env/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file)\"]\n"
     ]
    }
   ],
   "source": [
    "# Importing Packages\n",
    "import country_converter as coco\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importing partial packages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5c3aa1",
   "metadata": {},
   "source": [
    "## Defining Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ca5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing column names to standardize\n",
    "def standardize_column_names(dataframe):\n",
    "    dataframe.columns = [\n",
    "        str(column).lower().replace(\" \", \"_\").replace(\",\", \"\")\n",
    "        for column in dataframe.columns\n",
    "    ]\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfcc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(country):\n",
    "    if pd.isna(country):\n",
    "        return country\n",
    "    country = str(country).strip()\n",
    "    country = p.sub(\"\", country)\n",
    "    return re.sub(r\",\\s*The$\", \"\", country, flags=re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf2e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short(country):\n",
    "    if pd.isna(country):\n",
    "        return pd.NA\n",
    "    res = cc.convert(names=clean(country), to=\"name_short\")\n",
    "    if isinstance(res, (list, tuple)):\n",
    "        res = res[0] if res else pd.NA\n",
    "    if not res or res == \"not found\" or pd.isna(res):\n",
    "        warnings.warn(f\"Problem: {country}\")\n",
    "        return pd.NA\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a9ae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xxxusd(code):\n",
    "    x = f\"{code}USD=X\", f\"USD{code}=X\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b03401",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc415f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                   object\n",
       "slug                   object\n",
       "value                  object\n",
       "date_of_information     int64\n",
       "ranking                 int64\n",
       "region                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Real GDP Purchasing Power Parity\n",
    "# Note the Ranking; 1 = big country per region\n",
    "country_gdp = pd.read_csv(\"../data/input/real_gdp_purchasing_power_parity_0.csv\")\n",
    "\n",
    "# Displaying the first 5 rows of the dataframe\n",
    "country_gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Fred Anchors\n",
    "df_fred_world_gdp = pd.read_csv(\"../data/input/fred_anchors_0.csv\")\n",
    "\n",
    "# Preprocessing Fred Anchors\n",
    "df_fred_world_gdp[\"observation_date\"] = pd.to_datetime(\n",
    "    df_fred_world_gdp[\"observation_date\"]\n",
    ")\n",
    "df_fred_world_gdp[\"time_period\"] = df_fred_world_gdp[\"observation_date\"].dt.year\n",
    "df_fred_world_gdp = df_fred_world_gdp.drop(columns=[\"observation_date\"]).rename(\n",
    "    columns={\"NYGDPMKTPCDWLD\": \"world_gdp\"}\n",
    ")\n",
    "\n",
    "# Displaying the first 5 rows of the dataframe\n",
    "df_fred_world_gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d496e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing GDP\n",
    "df_imf_gdp = pd.read_csv(\n",
    "    \"../data/input/imf_gdp_0.csv\",\n",
    "    on_bad_lines=\"skip\",\n",
    "    sep=\",\",\n",
    "    quoting=csv.QUOTE_ALL,\n",
    "    skipinitialspace=True,\n",
    "    usecols=[\n",
    "        \"COUNTRY\",\n",
    "        \"TIME_PERIOD\",\n",
    "        \"TYPE_OF_TRANSFORMATION\",\n",
    "        \"FREQUENCY\",\n",
    "        \"OBS_VALUE\",\n",
    "        \"INDICATOR\",\n",
    "    ],\n",
    "    engine=\"python\",\n",
    ")\n",
    "\n",
    "# Preprocessing GDP\n",
    "df_imf_gdp = (\n",
    "    standardize_column_names(df_imf_gdp)\n",
    "    .query(\n",
    "        \"indicator == 'US Dollar per domestic currency' \"\n",
    "        \"and type_of_transformation == 'End-of-period (EoP)' \"\n",
    "        \"and frequency == 'Annual'\"\n",
    "    )\n",
    "    .dropna(subset=[\"obs_value\"])\n",
    "    .rename(columns={\"time_period\": \"year\"})\n",
    "    .assign(year=lambda d: d[\"year\"].str[:4])\n",
    "    .sort_values([\"country\", \"year\"])\n",
    "    .drop(columns=[\"indicator\", \"type_of_transformation\", \"frequency\"])\n",
    "    .reset_index(drop=True)\n",
    ").assign(year=lambda d: pd.to_datetime(d[\"year\"], errors=\"coerce\").dt.year)\n",
    "\n",
    "# Displaying the first 5 rows of the dataframe\n",
    "df_imf_gdp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5d450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Exchange Rates\n",
    "df_imf_trade = pd.read_csv(\n",
    "    \"../data/input/imf_trade_0.csv\",\n",
    "    usecols=[\n",
    "        \"COUNTRY\",\n",
    "        \"COUNTERPART_COUNTRY\",\n",
    "        \"TIME_PERIOD\",\n",
    "        \"OBS_VALUE\",\n",
    "        \"TRADE_FLOW\",\n",
    "        \"SCALE\",\n",
    "        \"UNIT\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Manually excluding countries that are either trade in pegged currencies or don't have a true boundary from another\n",
    "ls_0 = [\n",
    "    \"World\",\n",
    "    \"Advanced Economies\",\n",
    "    \"Latin America and the Caribbean (LAC)\",\n",
    "    \"Hong Kong Special Administrative Region, People's Republic of China\",\n",
    "    \"Emerging and Developing Europe\",\n",
    "    \"Middle East and Central Asia\",\n",
    "    \"Emerging Market and Developing Economies\",\n",
    "    \"Euro Area (EA)\",\n",
    "    \"Emerging and Developing Asia\",\n",
    "]\n",
    "\n",
    "# Excluding the \"Exclusion List\"\n",
    "df_imf_trade = df_imf_trade[\n",
    "    (~df_imf_trade[\"COUNTRY\"].isin(ls_0))\n",
    "    & (~df_imf_trade[\"COUNTERPART_COUNTRY\"].isin(ls_0))\n",
    "]\n",
    "\n",
    "# Viewing first five rows\n",
    "df_imf_trade = standardize_column_names(df_imf_trade).sort_values(\n",
    "    by=[\"obs_value\"], ascending=False\n",
    ")\n",
    "\n",
    "# Displaying the first 5 rows of the dataframe\n",
    "df_imf_trade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bda0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining countries in the European Union that use the Euro (EUR) and British Pound (GBP)\n",
    "EUR = [\n",
    "    \"Austria\",\n",
    "    \"Belgium\",\n",
    "    \"Cyprus\",\n",
    "    \"Estonia\",\n",
    "    \"Finland\",\n",
    "    \"France\",\n",
    "    \"Germany\",\n",
    "    \"Greece\",\n",
    "    \"Ireland\",\n",
    "    \"Italy\",\n",
    "    \"Latvia\",\n",
    "    \"Lithuania\",\n",
    "    \"Luxembourg\",\n",
    "    \"Malta\",\n",
    "    \"Netherlands\",\n",
    "    \"Portugal\",\n",
    "    \"Slovakia\",\n",
    "    \"Slovenia\",\n",
    "    \"Spain\",\n",
    "    \"Andorra\",\n",
    "    \"Monaco\",\n",
    "    \"San Marino\",\n",
    "    \"Vatican City\",\n",
    "    \"Saint Barthélemy\",\n",
    "    \"Saint Pierre and Miquelon\",\n",
    "    \"Kosovo\",\n",
    "    \"Montenegro\",\n",
    "    \"Bosnia and Herzegovina\",\n",
    "    \"Bulgaria\",\n",
    "    \"Cape Verde\",\n",
    "    \"Cameroon\",\n",
    "    \"Central African Republic\",\n",
    "    \"Chad\",\n",
    "    \"Republic of the Congo\",\n",
    "    \"Equatorial Guinea\",\n",
    "    \"Gabon\",\n",
    "    \"Benin\",\n",
    "    \"Burkina Faso\",\n",
    "    \"Côte d'Ivoire\",\n",
    "    \"Guinea-Bissau\",\n",
    "    \"Mali\",\n",
    "    \"Niger\",\n",
    "    \"Senegal\",\n",
    "    \"Togo\",\n",
    "    \"French Polynesia\",\n",
    "    \"New Caledonia\",\n",
    "    \"Wallis and Futuna\",\n",
    "    \"Comoros\",\n",
    "    \"Croatia\",\n",
    "    \"Morocco\",\n",
    "    \"São Tomé and Príncipe\",\n",
    "    \"Denmark\",\n",
    "    \"North Macedonia\",\n",
    "]\n",
    "\n",
    "# Defining countries that use the British Pound (GBP)\n",
    "GBP = [\n",
    "    \"Guernsey\",\n",
    "    \"Jersey\",\n",
    "    \"Isle of Man\",\n",
    "    \"Gibraltar\",\n",
    "    \"Falkland Islands\",\n",
    "    \"Saint Helena\",\n",
    "]\n",
    "\n",
    "# Defining other countries with pegged currencies or no true boundary\n",
    "other = [\n",
    "    \"Bhutan\",\n",
    "    \"Nepal\",\n",
    "    \"North Korea\",\n",
    "    \"Afghanistan\",\n",
    "    \"Turkmenistan\",\n",
    "    \"South Sudan\",\n",
    "    \"Guam\",\n",
    "    \"Macau\",\n",
    "    \"Tuvalu\",\n",
    "    \"Kiribati\",\n",
    "    \"Palau\",\n",
    "    \"Greenland\",\n",
    "    \"Maldives\",\n",
    "    \"Iraq\",\n",
    "    \"Solomon Islands\",\n",
    "    \"Brunei Darussalam\",\n",
    "    \"Bangladesh\",\n",
    "    \"Myanmar\",\n",
    "    \"Marshall Islands\",\n",
    "    \"Iran\",\n",
    "    \"Yemen\",\n",
    "    \"Libya\",\n",
    "    \"Somalia\",\n",
    "    \"Liberia\",\n",
    "    \"Sudan\",\n",
    "    \"Sierra Leone\",\n",
    "    \"Mongolia\",\n",
    "    \"Angola\",\n",
    "    \"Kyrgyz Republic\",\n",
    "    \"Tajikistan\",\n",
    "]\n",
    "\n",
    "# Cleaning country names\n",
    "EUR = {short(x) for x in EUR if pd.notna(short(x))}\n",
    "GBP = {short(x) for x in GBP if pd.notna(short(x))}\n",
    "other = {short(x) for x in other if pd.notna(short(x))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa99ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing cleaned data\n",
    "# TODO - where is this coming from? I.e. what code created these files?\n",
    "df_trade = pd.read_parquet(\"../data/input/trade_0.parquet\")\n",
    "df_gdp = pd.read_parquet(\"../data/input/gdp_0.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379e13d9",
   "metadata": {},
   "source": [
    "## Applying Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95afa563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting country names to short format\n",
    "cc = coco.CountryConverter()\n",
    "p = re.compile(\n",
    "    r\",\\s*(?:Kingdom of the Netherlands|United Kingdom-British Overseas Territory|Republic of the|Union of the|State of the)$\",\n",
    "    re.I,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6196915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchors selection\n",
    "df_gdp = df_gdp[~(df_gdp[\"COUNTRY\"] == \"United States\")]\n",
    "df_gdp.head()\n",
    "Anchor = {\n",
    "    \"EUR\": EUR,\n",
    "    \"CNY\": {\"China\"},\n",
    "    \"JPY\": {\"Japan\"},\n",
    "}\n",
    "\n",
    "# No USD due to many relationship ties with multiple currency.\n",
    "# Time Period\n",
    "period = 10\n",
    "\n",
    "# N Neighbor\n",
    "n = 10\n",
    "\n",
    "# Minimum Percentage Volume\n",
    "v = 0.05\n",
    "\n",
    "# Exports of goods\n",
    "# Neighbor mapping\n",
    "neighbor = {x: i for i, y in Anchor.items() for x in y}\n",
    "ex = EUR | GBP | other | {\"China\", \"Japan\", \"United States\"}\n",
    "df_trade = df_trade.dropna(\n",
    "    subset=[\"COUNTRY\", \"COUNTERPART_COUNTRY\", \"TIME_PERIOD\", \"OBS_VALUE\"]\n",
    ").copy()\n",
    "\n",
    "# Keep data period\n",
    "df_trade = df_trade[df_trade[\"TIME_PERIOD\"].astype(str).str.match(r\"^\\d{4}\")]\n",
    "df_trade[\"year\"] = df_trade[\"TIME_PERIOD\"].astype(str).str[:4].astype(int)\n",
    "max_year = int(df_trade[\"year\"].max())\n",
    "df_trade = df_trade[df_trade[\"year\"].between(max_year - period + 1, max_year)]\n",
    "\n",
    "# Filter and map potential neighbors\n",
    "df_export = df_trade[~df_trade[\"COUNTRY\"].isin(ex)].copy()\n",
    "df_export[\"anchor\"] = df_export[\"COUNTERPART_COUNTRY\"].map(neighbor)\n",
    "\n",
    "# Total exports per country\n",
    "etot = (\n",
    "    df_export.groupby(\"COUNTRY\", as_index=False)[\"OBS_VALUE\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"OBS_VALUE\": \"export_total\"})\n",
    ")\n",
    "\n",
    "# Exports to each anchor\n",
    "eanch = (\n",
    "    df_export.dropna(subset=[\"anchor\"])\n",
    "    .groupby([\"COUNTRY\", \"anchor\"], as_index=False)[\"OBS_VALUE\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"OBS_VALUE\": \"export_to_anchor\"})\n",
    ")\n",
    "\n",
    "# Export shares\n",
    "esh = eanch.merge(etot, on=\"COUNTRY\", how=\"left\").assign(\n",
    "    export_share=lambda d: d[\"export_to_anchor\"] / d[\"export_total\"]\n",
    ")\n",
    "\n",
    "# Best export anchor per country\n",
    "bexp = esh.sort_values(\n",
    "    [\"COUNTRY\", \"export_share\", \"export_to_anchor\"], ascending=[True, False, False]\n",
    ").drop_duplicates(\"COUNTRY\")\n",
    "bexp = bexp[bexp[\"export_share\"] >= v]\n",
    "\n",
    "# Swap role\n",
    "df_import = df_trade.rename(\n",
    "    columns={\"COUNTRY\": \"COUNTERPART_COUNTRY_orig\", \"COUNTERPART_COUNTRY\": \"COUNTRY\"}\n",
    ").rename(columns={\"COUNTERPART_COUNTRY_orig\": \"COUNTERPART_COUNTRY\"})\n",
    "\n",
    "# Potential neighbors\n",
    "df_import = df_import[~df_import[\"COUNTRY\"].isin(ex)].copy()\n",
    "df_import[\"anchor\"] = df_import[\"COUNTERPART_COUNTRY\"].map(neighbor)\n",
    "\n",
    "# Total imports per country\n",
    "import_totals = (\n",
    "    df_import.groupby(\"COUNTRY\", as_index=False)[\"OBS_VALUE\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"OBS_VALUE\": \"import_total\"})\n",
    ")\n",
    "\n",
    "# Imports from each anchor\n",
    "ianch = (\n",
    "    df_import.dropna(subset=[\"anchor\"])\n",
    "    .groupby([\"COUNTRY\", \"anchor\"], as_index=False)[\"OBS_VALUE\"]\n",
    "    .sum()\n",
    "    .rename(columns={\"OBS_VALUE\": \"import_from_anchor\"})\n",
    ")\n",
    "\n",
    "# Import shares\n",
    "ish = ianch.merge(import_totals, on=\"COUNTRY\", how=\"left\").assign(\n",
    "    import_share=lambda d: d[\"import_from_anchor\"] / d[\"import_total\"]\n",
    ")\n",
    "\n",
    "# Best import anchor per country\n",
    "bimp = ish.sort_values(\n",
    "    [\"COUNTRY\", \"import_share\", \"import_from_anchor\"], ascending=[True, False, False]\n",
    ").drop_duplicates(\"COUNTRY\")\n",
    "bimp = bimp[bimp[\"import_share\"] >= v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine table\n",
    "comb = esh.merge(ish, on=[\"COUNTRY\", \"anchor\"], how=\"outer\", suffixes=(\"_exp\", \"_imp\"))\n",
    "\n",
    "# Handle missing value\n",
    "f = [\n",
    "    \"export_to_anchor\",\n",
    "    \"export_total\",\n",
    "    \"export_share\",\n",
    "    \"import_from_anchor\",\n",
    "    \"import_total\",\n",
    "    \"import_share\",\n",
    "]\n",
    "for c in f:\n",
    "    if c not in comb.columns:\n",
    "        comb[c] = 0.0\n",
    "comb[f] = comb[f].fillna(0.0)\n",
    "\n",
    "# Combined metrics\n",
    "comb = comb.assign(\n",
    "    total_trade_with_anchor=lambda d: d[\"export_to_anchor\"] + d[\"import_from_anchor\"],\n",
    "    total_trade_volume=lambda d: d[\"export_total\"] + d[\"import_total\"],\n",
    ")\n",
    "comb[\"combined_exposure\"] = 0.0\n",
    "nz = comb[\"total_trade_volume\"] > 0\n",
    "comb.loc[nz, \"combined_exposure\"] = (\n",
    "    comb.loc[nz, \"total_trade_with_anchor\"] / comb.loc[nz, \"total_trade_volume\"]\n",
    ")\n",
    "comb_f = comb.loc[comb[\"combined_exposure\"] >= v].copy()\n",
    "\n",
    "# Rank neighbors per anchor\n",
    "comb_f = comb_f.sort_values(\n",
    "    by=[\n",
    "        \"anchor\",\n",
    "        \"combined_exposure\",\n",
    "        \"total_trade_with_anchor\",\n",
    "        \"export_share\",\n",
    "        \"import_share\",\n",
    "        \"COUNTRY\",\n",
    "    ],\n",
    "    ascending=[True, False, False, False, False, True],\n",
    ")\n",
    "\n",
    "# Rank within each anchor and take top n\n",
    "comb_f[\"rank_within_anchor\"] = comb_f.groupby(\"anchor\")[\"combined_exposure\"].rank(\n",
    "    method=\"first\", ascending=False\n",
    ")\n",
    "comb_top = comb_f.loc[comb_f[\"rank_within_anchor\"] <= n].copy()\n",
    "\n",
    "# Final neighbor\n",
    "neighbors_dict = comb_top.groupby(\"anchor\")[\"COUNTRY\"].apply(list).to_dict()\n",
    "\n",
    "# Summary results\n",
    "summary = (\n",
    "    comb_top[\n",
    "        [\n",
    "            \"anchor\",\n",
    "            \"COUNTRY\",\n",
    "            \"combined_exposure\",\n",
    "            \"export_share\",\n",
    "            \"import_share\",\n",
    "            \"export_to_anchor\",\n",
    "            \"import_from_anchor\",\n",
    "            \"export_total\",\n",
    "            \"import_total\",\n",
    "            \"total_trade_with_anchor\",\n",
    "            \"total_trade_volume\",\n",
    "            \"rank_within_anchor\",\n",
    "        ]\n",
    "    ]\n",
    "    .sort_values([\"anchor\", \"rank_within_anchor\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "res = summary.copy()\n",
    "for col in [\"combined_exposure\", \"export_share\", \"import_share\"]:\n",
    "    res[col] = (res[col] * 100).round(2)\n",
    "\n",
    "res_sorted = res.sort_values([\"anchor\", \"rank_within_anchor\"])\n",
    "anchor_neighbor = res_sorted.groupby(\"anchor\")[\"COUNTRY\"].apply(list).to_dict()\n",
    "\n",
    "for a, c in [(\"CNY\", \"Australia\"), (\"JPY\", \"Philippines\"), (\"JPY\", \"Vietnam\")]:\n",
    "    if a in anchor_neighbor and c in anchor_neighbor[a]:\n",
    "        anchor_neighbor[a].remove(c)\n",
    "\n",
    "anchor_neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3e8c5",
   "metadata": {},
   "source": [
    "*Performing Principle Component Analyis (PCA) to reduce the dimensionality of our training data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1db890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We identify the hour anchor: Anchor = {\"EUR\": EUR, \"CNY\": {\"China\"}, \"JPY\": {\"Japan\"}}\n",
    "\n",
    "# Going to start by pulling Anchor Countries\n",
    "# Codes are obtain from yahoo finance symbol\n",
    "# https://finance.yahoo.com/markets/currencies/\n",
    "\n",
    "fx_list = {\n",
    "    # Anchor currencies\n",
    "    \"CNY\": \"CNY=X\",  # China\n",
    "    \"EUR\": \"EUR=X\",  # Euro\n",
    "    \"JPY\": \"JPY=X\",  # Japan\n",
    "    # Neutral currencies\n",
    "    \"CAD\": \"CAD=X\",  # Canada\n",
    "    \"BRL\": \"BRL=X\",  # Brazil\n",
    "    \"MXN\": \"MXN=X\",  # Mexico\n",
    "    \"COP\": \"COP=X\",  # Colombia\n",
    "    \"PEN\": \"PEN=X\",  # Peru\n",
    "    \"NOK\": \"NOK=X\",  # Norway\n",
    "    \"ZAR\": \"ZAR=X\",  # South Africa\n",
    "    \"INR\": \"INR=X\",  # India\n",
    "    \"TRY\": \"TRY=X\",  # Turkey\n",
    "    \"EGP\": \"EGP=X\",  # Egypt\n",
    "    \"RUB\": \"RUB=X\",  # Russia\n",
    "    \"ILS\": \"ILS=X\",  # Israel\n",
    "    # CNY group\n",
    "    \"CDF\": \"CDF=X\",  # DR Congo\n",
    "    \"LAK\": \"LAK=X\",  # Laos\n",
    "    \"TZS\": \"TZS=X\",  # Tanzania\n",
    "    \"CLP\": \"CLP=X\",  # Chile\n",
    "    \"GNF\": \"GNF=X\",  # Guinea\n",
    "    \"PKR\": \"PKR=X\",  # Pakistan\n",
    "    \"PHP\": \"PHP=X\",  # Philippines\n",
    "    \"VND\": \"VND=X\",  # Vietnam\n",
    "    \"MRU\": \"MRU=X\",  # Mauritania\n",
    "    # EUR group\n",
    "    \"ALL\": \"ALL=X\",  # Albania\n",
    "    \"CZK\": \"CZK=X\",  # Czechia\n",
    "    \"TND\": \"TND=X\",  # Tunisia\n",
    "    \"RON\": \"RON=X\",  # Romania\n",
    "    \"HUF\": \"HUF=X\",  # Hungary\n",
    "    \"PLN\": \"PLN=X\",  # Poland\n",
    "    \"RSD\": \"RSD=X\",  # Serbia\n",
    "    \"SEK\": \"SEK=X\",  # Sweden\n",
    "    \"ISK\": \"ISK=X\",  # Iceland\n",
    "    \"DZD\": \"DZD=X\",  # Algeria\n",
    "    # JPY group\n",
    "    \"PGK\": \"PGK=X\",  # Papua New Guinea\n",
    "    \"TWD\": \"TWD=X\",  # Taiwan\n",
    "    \"THB\": \"THB=X\",  # Thailand\n",
    "    \"AUD\": \"AUD=X\",  # Australia\n",
    "    \"IDR\": \"IDR=X\",  # Indonesia\n",
    "    \"KRW\": \"KRW=X\",  # South Korea\n",
    "    \"MYR\": \"MYR=X\",  # Malaysia\n",
    "    \"NZD\": \"NZD=X\",  # New Zealand\n",
    "}\n",
    "\n",
    "# Period is the timeframe we want, we conclude for the time being, it will be 10 years\n",
    "PERIOD = \"10y\"\n",
    "\n",
    "# We conclude we will be looking at exchanges on a daily basis.\n",
    "INTERVAL = \"1d\"\n",
    "FFill_Limit = 3\n",
    "N_PCS_To_Cluster = 3\n",
    "N_Cluster = 3\n",
    "\n",
    "# Creating a list of tickers from the fx_list dictionary\n",
    "tickers = list(fx_list.values())\n",
    "\n",
    "# Creating a DataFrame by downloading historical FX data using yfinance\n",
    "yfinance_fx_raw = yf.download(\n",
    "    tickers, period=PERIOD, interval=INTERVAL, auto_adjust=None, progress=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85bdfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove problem currency\n",
    "x = yfinance_fx_raw[\"Close\"]\n",
    "c = x.notna().sum().sort_values()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove currencies that have less than 2000 data points\n",
    "want = list(fx_list.keys())\n",
    "t = []\n",
    "invert = {}\n",
    "\n",
    "# Loop through each currency in the want list and get their corresponding ticker symbols\n",
    "for c in want:\n",
    "    t1, t2 = xxxusd(c)\n",
    "    t.extend([t1, t2])\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "t = list(dict.fromkeys(t))\n",
    "\n",
    "# Creating a DataFrame by downloading historical FX data using yfinance\n",
    "raw = yf.download(\n",
    "    t, period=PERIOD, interval=INTERVAL, auto_adjust=None, progress=False\n",
    ")[\"Close\"]\n",
    "\n",
    "# Creating a DataFrame to hold exchange rate levels and whether they need to be inverted\n",
    "level = {}\n",
    "for c in want:\n",
    "    t1, t2 = xxxusd(c)\n",
    "    if t1 in raw.columns and raw[t1].notna().sum() > 0:\n",
    "        level[c] = raw[t1]\n",
    "        invert[c] = False\n",
    "    elif t2 in raw.columns and raw[t2].notna().sum() > 0:\n",
    "        level[c] = 1.0 / raw[t2]\n",
    "        invert[c] = True\n",
    "\n",
    "# Creating a DataFrame from the level dictionary and sorting by index\n",
    "level = pd.DataFrame(level).sort_index()\n",
    "\n",
    "# Calculating log returns and handling missing values\n",
    "lreturn = np.log(level / level.shift(1))\n",
    "lreturn = lreturn.ffill(limit=3).dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0238da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the log returns\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(\n",
    "    scaler.fit_transform(lreturn),\n",
    "    index=lreturn.index,\n",
    "    columns=lreturn.columns,\n",
    ")\n",
    "\n",
    "# PCA and KMeans Clustering\n",
    "pca = PCA(n_components=2)\n",
    "Z = pca.fit_transform(X.T)\n",
    "\n",
    "# Creating a DataFrame for PCA results\n",
    "pca_df = pd.DataFrame(Z, index=X.columns, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "# KMeans Clustering\n",
    "k = 3\n",
    "km = KMeans(n_clusters=k, random_state=42)\n",
    "pca_df[\"cluster\"] = km.fit_predict(pca_df[[\"PC1\", \"PC2\"]].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c1db53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addidtional pull for explained_variance_ratio_\n",
    "evr = pca.explained_variance_ratio_\n",
    "print(\"Explained variance ratio: \", evr, \" | Cumalative: \", evr.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597912e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping currencies based on economic ties\n",
    "g = {\n",
    "    \"CNY\": [\"CDF\", \"LAK\", \"TZS\", \"CLP\", \"GNF\", \"PKR\", \"PHP\", \"VND\", \"MRU\"],\n",
    "    \"EUR\": [\"ALL\", \"CZK\", \"TND\", \"RON\", \"HUF\", \"PLN\", \"RSD\", \"SEK\", \"ISK\", \"DZD\"],\n",
    "    \"JPY\": [\"PGK\", \"TWD\", \"THB\", \"AUD\", \"IDR\", \"KRW\", \"MYR\", \"NZD\"],\n",
    "}\n",
    "\n",
    "\n",
    "# Function to assign group based on currency code\n",
    "def assign(cc):\n",
    "    for i, j in g.items():\n",
    "        if cc == i or cc in j:\n",
    "            return i\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "# Assigning group to each currency in the PCA DataFrame\n",
    "pca_df[\"group\"] = pca_df.index.map(assign)\n",
    "\n",
    "# Visualization of PCA Results\n",
    "col = {\"CNY\": \"red\", \"EUR\": \"blue\", \"INR\": \"green\", \"JPY\": \"orange\", \"other\": \"gray\"}\n",
    "plt.figure(figsize=(12, 8))\n",
    "for grp, sub in pca_df.groupby(\"group\"):\n",
    "    plt.scatter(\n",
    "        sub[\"PC1\"],\n",
    "        sub[\"PC2\"],\n",
    "        c=col.get(grp, \"black\"),\n",
    "        s=70,\n",
    "        alpha=0.85,\n",
    "        label=f\"{grp} (n={len(sub)})\",\n",
    "    )\n",
    "    for i, j in sub.iterrows():\n",
    "        plt.annotate(\n",
    "            i,\n",
    "            (j[\"PC1\"], j[\"PC2\"]),\n",
    "            xytext=(3, 3),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "plt.title(\"Currencies in PCA Space (standardized returns, XXX/USD)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59125e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Silhouette Score\n",
    "score = silhouette_score(pca_df[['PC1','PC2']], pca_df['cluster'])\n",
    "\n",
    "# Saving the log returns DataFrame to a Parquet file\n",
    "lreturn.to_parquet(\"fx_log_return.parquet\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e4ed2",
   "metadata": {},
   "source": [
    "## Applying Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd90a84",
   "metadata": {},
   "source": [
    "*Applying Granger causality test to determine if a causal realtionship is found in our anchor and neighboring currency returns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651123e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m lreturn = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/input/fx_log_return.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m g = {\u001b[33m\"\u001b[39m\u001b[33mCNY\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mCDF\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mLAK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTZS\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCLP\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mGNF\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPKR\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPHP\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mVND\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMRU\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      4\u001b[39m      \u001b[33m\"\u001b[39m\u001b[33mEUR\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mALL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCZK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTND\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRON\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHUF\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPLN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRSD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSEK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mISK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDZD\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      5\u001b[39m      \u001b[33m\"\u001b[39m\u001b[33mJPY\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mPGK\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTWD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTHB\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAUD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIDR\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mKRW\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMYR\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNZD\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgranger\u001b[39m(res, g, ml=\u001b[32m5\u001b[39m, a=\u001b[32m0.05\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/pandas/io/parquet.py:653\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    502\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     **kwargs,\n\u001b[32m    511\u001b[39m ) -> DataFrame:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    514\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    651\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    656\u001b[39m         msg = (\n\u001b[32m    657\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    658\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    659\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/siads-696/env/lib/python3.12/site-packages/pandas/io/parquet.py:68\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m             error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA suitable version of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to import the above resulted in these errors:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[31mImportError\u001b[39m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "lreturn = pd.read_parquet('../data/input/fx_log_return.parquet')\n",
    "\n",
    "g = {\"CNY\": [\"CDF\", \"LAK\", \"TZS\", \"CLP\", \"GNF\", \"PKR\", \"PHP\", \"VND\", \"MRU\"],\n",
    "     \"EUR\": [\"ALL\", \"CZK\", \"TND\", \"RON\", \"HUF\", \"PLN\", \"RSD\", \"SEK\", \"ISK\", \"DZD\"],\n",
    "     \"JPY\": [\"PGK\", \"TWD\", \"THB\", \"AUD\", \"IDR\", \"KRW\", \"MYR\", \"NZD\"]}\n",
    "\n",
    "def granger(res, g, ml=5, a=0.05):\n",
    "    x = []\n",
    "    for i, j in g.items():\n",
    "        for k in j:\n",
    "            if i not in res.columns or k not in res.columns:\n",
    "                continue\n",
    "            df = pd.concat([res[k], res[i]], axis=1).dropna()\n",
    "            df.columns = [\"neighbor\", \"anchor\"]\n",
    "            if len(df) <= ml * 3:\n",
    "                continue\n",
    "            t = grangercausalitytests(df, maxlag=ml)\n",
    "            pval = [t[lag][0]['ssr_ftest'][1] for lag in range(1, ml + 1)]\n",
    "            min_p = float(np.min(pval))\n",
    "            best_lag = int(np.argmin(pval) + 1)\n",
    "            x.append({\"anchor\": i,\n",
    "                    \"neighbor\": k,\n",
    "                    \"min_pval\": round(min_p, 4),\n",
    "                    \"best_lag (days)\": best_lag,\n",
    "                    \"significant\": (min_p < a)})\n",
    "    rdf = pd.DataFrame(x)\n",
    "    return rdf\n",
    "\n",
    "rdf = granger(lreturn,g,10,0.05)\n",
    "rdf = rdf.sort_values([\"anchor\", \"min_pval\"]).reset_index(drop=True)\n",
    "an = rdf[rdf[\"significant\"]==True].sort_values([\"anchor\", \"min_pval\"])\n",
    "ng = (an.groupby(\"anchor\")[\"neighbor\"].apply(list).to_dict())\n",
    "ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4646da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_0 = [i for i in ng.items() if i[0] == 'EUR'][0]\n",
    "ls_1 = [ls_0[0] for i in range(len([i for i in ng.items() if i[0] == 'EUR'][0][1]))]\n",
    "len(ls_0[1]), len(ls_1)\n",
    "\n",
    "pd.DataFrame({'Anchor': ls_1, 'Neighbor': ls_0[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_0 = pd.read_parquet('../data/input/fx_log_return.parquet')[['CNY', 'CDF']]\n",
    "\n",
    "# Starting with CNY/CDF as an example\n",
    "df_0['target'] = df_0['CDF'].shift(-1)\n",
    "\n",
    "# Dropping all NA\n",
    "df_0.dropna(inplace=True)\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab891aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features (e.g., previous 5 days' returns)\n",
    "for i in range(1, 6):\n",
    "    df_0[f'Return_lag_{i}'] = df_0['CDF'].pct_change().shift(i)\n",
    "\n",
    "# Create moving average features\n",
    "df_0['MA_10'] = df_0['CDF'].rolling(window=10).mean().shift(1)\n",
    "df_0['MA_50'] = df_0['CDF'].rolling(window=50).mean().shift(1)\n",
    "\n",
    "# Drop initial rows with NaNs created by rolling windows\n",
    "df_0.dropna(inplace=True)\n",
    "df_0 = df_0.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "df_0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dabdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "features = [col for col in df_0.columns if col not in ['target']]\n",
    "X = df_0[features]\n",
    "y = df_0['target']\n",
    "\n",
    "# Time-based train-test split\n",
    "split_index = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "xgb = XGBRegressor()\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:squarederror'],\n",
    "              'learning_rate': [.03, .24, .8],\n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'subsample': [.2, .5, .8],\n",
    "              'colsample_bytree': [.7],\n",
    "              'n_estimators': [2, 5, 10],\n",
    "              }\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(X_train,\n",
    "             y_train)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
